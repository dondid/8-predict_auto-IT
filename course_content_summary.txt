=== 1. Introduction to ML.pdf ===
Ruxandra Stoean
rstoean@inf.ucv.ro
ruxandra.stoean@e-uvt.ro
Introduction to Machine Learning
Intelligent machine
ÔÇó A machine that can be programmed to act like a human
ÔÇó A machine that can be taught to learn like a human

What is Machine Learning?
Definitions
ÔÇó ‚ÄúThe field of study that gives computers the ability to 
learn without being explicitly programmed.‚Äù                  
‚Äì Arthur Samuel (1959)
ÔÇó ‚ÄúA computer program is said to learn from experience E 
with respect to some class of tasks T and performance 
measure P , if its performance at tasks in T , as measured 
by P , improves with experience E.‚Äù                                   
‚Äì Tom M. Mitchell (1999) 
ÔÇó ‚ÄúA subset of artificial intelligence focused on developing algorithms and
statistical models that enable computers to learn from data and improve
their performance on specific tasks without explicit programming.
ÔÇó It involves collecting and preparing data, designing mathematical models,
training the models on labeled data, evaluating their performance on new
data, and deploying them for predictions or decision-making.
ÔÇó It encompasses supervised learning (learning from labeled data),
unsupervised learning (discovering patterns in unlabeled data), and
reinforcement learning (learning through interaction with an
environment).
ÔÇó Its applications span a wide range of fields, revolutionizing technology
and problem-solving across domains like image recognition, natural
language processing, recommendation systems, and autonomous
vehicles.‚Äù
‚Äì ChatGPT
https://stablediffusionweb.com/

Applications. Autonomous vehicles
ÔÇó Waymo from Google
ÔÇó https://www .youtube.com/watch?v=0kJPDg207oc
ÔÇó Microsoft
ÔÇó https://www .youtube.com/watch?v=zkfrkgtrLdo
Medical diagnosis support
ÔÇó MedGemma - Hugging Face
ÔÇó Image classification, text analysis, report generation, patient triage
ÔÇó https://huggingface.co/spaces/google/appoint-ready
ÔÇó Microsoft AI Diagnostic Orchestrator (MAI-DxO)
ÔÇó 304 initial real cases from the New England Jou

=== 2. Supervised learning. Linear models.pdf ===
Ruxandra Stoean
Supervised learning. Linear models
Supervised learning
ÔÇó Concept
ÔÇó Input data with available output values
ÔÇó Learn the input ‚Äì output association
ÔÇó Prediction on the target of new data
ÔÇó Classification
ÔÇó Output grouped in two or more classes
ÔÇó Qualitative output (discrete, factor, categorical, ordered 
categorical)
ÔÇó Prediction on the class of new data
ÔÇó Regression
ÔÇó Quantitative output (continuous)
ÔÇó Prediction on the target numerical value for new input
Classification
A
A
B
?
Input Output
Association
Regression
0.2
?
Input Output
Association
0.4
0.9
Classification, 
regression
Training data (known 
output)
T est data 
(unknown output)
ModelLearning
P
r
e
d
i
c
t
i
o
n
Supervised learning
ÔÇó A data set - pairs of the type (input, output)
ÔÇó The input is a sequence of values for the data attributes
ÔÇó The output is a confirmed decision
ÔÇó Every record (object, example, vector) is described by a number 
of attributes with values from a discrete or continuous domain.
ÔÇó The targets are either all discrete (classification), or continuous 
(regression).
Definitions
ÔÇó Given a data set of m records {x1, x2, ‚Ä¶, xm}, where 
ÔÇó every data point is defined by n attributes ùê±ùê¢ ‚àà ùëÖùëõ 
ÔÇó each has an associated outcome y1, ‚Ä¶, ym
ÔÇó Discrete outcome y -> classification
ÔÇó Continuous outcome y -> regression
ÔÇó A supervised learning task is to build a model that 
ÔÇó learns the association between x and y
ÔÇó and predicts the outcome for new data points.
The model and the data
ÔÇó The data set is split in three distinct subsets:
ÔÇó The training set
ÔÇó The validation set
ÔÇó The test set (data output is hidden)
ÔÇó The algorithm learns the association between every training data 
point and its output (training phase).
ÔÇó The obtained model: 
ÔÇó tested on the validation set to measure its prediction error
ÔÇó Parameter tuning
ÔÇó Variable selection
ÔÇó tested on the test set to assess the generalization ability
TESTTRAINING VALIDATION
The loss function
ÔÇó Loss measures how much the model learns the I

=== 3. Support vector machines.pdf ===
Support vector machines (SVM)
Ruxandra Stoean
Additional bibliography
ÔÇó Simon O. Haykin , Neural Networks and Learning Machines
(3rd Edition), Prentice Hall, 2008
ÔÇó Bernhard Scholkopf, Alexander J. Smola, Learning with
Kernels: Support V ector Machines, Regularization,
Optimization, and Beyond, MIT Press, 2018
ÔÇó Catalin Stoean, Ruxandra Stoean, Support V ectorMachines
and Evolutionary Algorithms for Classification: Single or
T ogether?,Intelligent Systems Reference Library,Volume 69,
Springer, 2014.
What is an SVM?
ÔÇó V erypowerful tools in:
ÔÇó Classification
ÔÇó Handwriting recognition
ÔÇó Object recognition
ÔÇó Speech recognition
ÔÇó T ext categorization
ÔÇó Generically binary ‚Äì methods to extend to
more classes
ÔÇó Regression
ÔÇó A type of supervised learning machine
‚Ä¢ Training phase: learning 
(example   ->   label)
‚Ä¢ Test phase: prediction
(new example  ->     ?)
Binary classification
ÔÇó A training data set
ÔÇó A family of functions
ÔÇó Find parameters t to learn the optimal correspondence between
every x and y
xi yi (-1 or 1)f
t
ÔÅª miii yx ,...,2,1)},( =
n
i Rx ÔÉé
}1,1{ +‚àíÔÉéiy
ÔÅª ÔÅΩTtft ÔÉé|
}1,1{: +‚àí‚ÜíÔÉÇ n
tf
Learning within SVM
Linear separable
Linear non-separable
Linear support vector machines
Nonlinear support vector machines
denotes   +1
denotes   -1
Linear support vector machines
x = Number of tasks solved for homework
y = Hours 
spent on 
final 
project
Problem: Will I get a good mark in this exam? Y es
No
Linear SVM for separable data
How can
these data be
classified?
Any of these could be
good‚Ä¶
But which one is the
best?
denotes   +1
denotes   -1
Linear SVM for separable data
How can
these data be
classified?
Any of these could be
good‚Ä¶
But which one is the
best?
denotes   +1
denotes   -1
Problem: Many solutions! 
Some very bad.
Task: From available data, 
select the hyperplane that 
separates well ‚Äúin general‚Äù
Linear SVM for separable data
The existence of a hyperplane of 
separation with the equation:
0, =‚àí bxw
w
w
b
ÔÅª ÔÅΩ0| =‚àíÔÉó bxwx
denotes   +1
denotes   -1
‚Ä¢ w ‚Äì vector of weig

=== 4. Neural networks.pdf ===
Ruxandra Stoean
Neural networks (NN)
Additional bibliography
ÔÇó Simon O. Haykin , Neural Networks and Learning Machines 
(3rd Edition), Prentice Hall, 2008
ÔÇó Charu C. Aggarwal, Neural Networks and Deep Learning: A 
T extbook, Springer, 2018
ÔÇó Christopher M. Bishop, Neural Networks for Pattern 
Recognition, Oxford University Press, 1996
Modelling the human brain
ÔÇó Artificial neural networks simulate the mode of neural 
interaction within the human brain, directed towards 
learning:
ÔÇó Base units ‚Äì neurons
ÔÇó Connections between them ‚Äì synapses
ÔÇó The intensities (weights) of the synapses determine the 
performance of learning.

Artificial neuron McCulloch-Pitts
ÔÇó x ‚Äì input vector
ÔÇó w ‚Äì synaptic 
weights
ÔÇó b ‚Äì bias
x1
x2
xn
ùëß = ùë§ ‚àô ùë• + ùëè
w1
w2
wn
ÔÇó z ‚Äì linear combination unit
ÔÇó ùúë ‚Äì activation function of the neuron
ÔÇó Output 
ùúë(ùëß)
ùúë(ùëß)
Structure of a neural network
ÔÇó Input neurons (units) ‚Äì input layer (input data attributes)
ÔÇó Hidden neurons in the black-box of learning (inside data 
components to be learnt)
ÔÇó In one or more hidden layers
ÔÇó The output of a layer becomes the input for the next layer
ÔÇó Output neurons ‚Äì output layer (network output - classes/response)
ÔÇó The (supervised) learning starts from the problem data and 
optimizes the weights of the neurons on the base of the difference 
between the predicted and the real response.
Simple NN architecture
Input layer
Hidden layer
Output layer
ÔÇó Example with 1 hidden 
layer, 3 input neurons, 4 
units in the hidden layer and 
1 output
General NN architecture
https://stanford.edu/~shervine/teaching/cs-229
NN Flow
ÔÇó Batches of input data are given in the input layer
ÔÇó Batch size ‚Äì amount of data passed in one forward pass iteration
ÔÇó Neurons are multiplied by their weights
ÔÇó The product is further transformed by the activation function
ÔÇó The new collection of neurons are fed to the next layer
ÔÇó The output of the last layer ‚Äì the prediction ‚Äì is compared to the 
output -> the loss -> to be minimized
ÔÇó W eights are initiall

=== 5.1 Decision trees.pdf ===
Ruxandra Stoean
Decision Trees (DT)
Additional bibliography
ÔÇó Leo Breiman, Jerome Friedman, Charles J. Stone, R.A.
Olshen, Classification and Regression Trees, Chapman and
Hall/CRC, 1984
ÔÇó Clinton Sheppard, Tree-based Machine Learning Algorithms:
DecisionTrees,Random Forests, and Boosting, 2017
ÔÇó Lior Rokach, Oded Maimon, Data Mining with Decision Trees:
Theory and Applications (Second Edition), W orldScientific,
2014
Introduction
ÔÇó V ery appreciated technique :
ÔÇó Possibility to visualize the model learning the data
ÔÇó White-box learning, as opposed to support vector machines, 
neural networks (black box)
ÔÇó Data type independent
ÔÇó The learning procedure: 
ÔÇó Training data are recursively partitioned
ÔÇó Process stops when homogeneous terminal nodes are obtained 
(classes for classification or thresholds for regression)

Algorithms
ÔÇó ID3 ‚Äì categorical features; greedy split by largest
information gain for classification; pruning after tree
completion
ÔÇó C4.5 ‚Äì in addition, also for continuous features, by
partitioning into discrete intervals; output converted to IF-
THEN rules, rule accuracy gives order of application
ÔÇó C5.0 ‚Äì increased accuracy; smaller rule sets
ÔÇó CART ‚Äì in addition, also for regression; no rule sets
DT structure
ÔÇó Every internal node expresses a test in relation to an 
attribute of the problem, which partitions the objects.
ÔÇó Attributes: discrete or continuous
ÔÇó If continuous, a threshold is used for partitioning
ÔÇó Every edge signifies the partition resulting after the 
application of the test that corresponds to that node
ÔÇó T erminal (leaf) nodes represent the dominant class/group 
that resulted on that branch
Node split criterion
ÔÇó The measure of an efficient partitioning of the data in the 
current node, on the basis of a pair (attribute, value), is given 
by the following criteria: 
ÔÇó Classification
ÔÇó Gini index -> minimum
ÔÇó CART
ÔÇó Information gain -> maximum
ÔÇó i.e., entropy -> minimum
ÔÇó ID3, C4.5, C5.0
ÔÇó Regression
ÔÇó Sum of squared errors
Algorith

=== 5.2 Ensemble methods.pdf ===
Ensemble learning
Ruxandra Stoean
Further bibliography
ÔÇó Leo Breiman, Bagging Predictors, Machine Learning , vol. 24,
issue 2, pp. 123-140, 1996
ÔÇó Y oav Freund, Rober E. Schapire, Experiments with a new
boosting algorithm, 13th International Conference on Machine
Learning, 148‚Äì156,Morgan Kaufmann, 1996
ÔÇó Leo Breiman, Random Forests, Machine Learning 45(1), 5-32,
2001.
ÔÇó Gautam Kunapuli, Ensemble Methods for Machine Learning,
Manning, 2023
Ensemble learning
ÔÇó Several base learners are trained and their predictions are
combined.
ÔÇó The way of application of the learners may take place at
several levels.
ÔÇó Classical approaches:
ÔÇó Bagging
ÔÇó Boosting
ÔÇó Random forests
medium.com
Bagging
ÔÇó Short for Bootstrap AGGregatING.
ÔÇó The training data set of m samples is split into b parts (bags).
ÔÇó Every subset has the same number of m elements.
ÔÇó The data from each subset are sampled with replacement.
ÔÇó The base learner is trained on each subset.
ÔÇó The b constructed models vote the output for new test data:
ÔÇó The mean of the outputs for the b models - for regression
ÔÇó The class with most appearances in the predictions of the b models - 
for classification
ÔÇó Examples not selected (out-of-bag) are used to estimate the
generalization error.
Package R ipred
ÔÇó The base learner is a decision tree (bagged tree).
ÔÇó In this sense package rpart is called.
ÔÇó If the output is seen as factor, classification is considered.
ÔÇó If it is continuous, regression is in place.
ÔÇó Parameter nbagg specifies the desired number of bags ‚Äì default 
25.
ÔÇó m samples are taken in each bag
ÔÇó Parameter coob=true specifies the scope to compute an 
estimation of the generalization error (out-of-bag estimation)
Code

Result

Python. Data processing WDBC
ÔÇó For the Wisconsin original data set with 9 attributes and 699 
records, we will have to read the .csv file.
ÔÇó In the collection available from UCI, missing values are 
denoted by ‚Äò?‚Äô
ÔÇó Their transformation to 0 will be performed through the use 
of applymap and the a

=== 6. Performance evaluation.pdf ===
Performance evaluation
Ruxandra Stoean
Further bibliography
ÔÇó N. Japkowicz, M. Shah, Evaluating Learning Algorithms,
Cambridge University Press, 2011
ÔÇó T. Hastie, R. Tibshirani, J. Friedman, The Elements of
Statistical Learning, Springer Series in Statistics, 2001
ÔÇó C. Huyen, Designing Machine Learning Systems: An Iterative 
Process for Production-Ready Applications, O'Reilly, 2022
Directions
ÔÇó Performance measures
ÔÇó Classification
ÔÇó Regression
ÔÇó Error estimation
ÔÇó Statistical significance
Accuracy / Error rate
ÔÇó The two complementary measures give an insight over the 
general performance.
ÔÇó Accuracy is computed as the ratio between the amount of data 
labelled correctly and the total number of examples 
ÔÇó The error rate is the ratio between the amount of samples labelled 
incorrectly and the total number of samples
ÔÇó Disadvantages in the cases when:
ÔÇó Data with class imbalance
ÔÇó Different misclassification costs for each class
Confusion matrix
ÔÇó For k classes, a matrix of k √ó k
ÔÇó The element at position (i, j) gives the amount of data of class 
i that the classifier labels as j
ÔÇó Generically, for two classes (positive and negative labels)
ÔÇó N = TN + FP , P = FN + TP
Predicted negative Predicted positive
Real negative True negative (TN) False positive (FP)
Real positive False negative (FN) True positive (TP)
Performance measures for a single 
class of interest 1/2
ÔÇó Class of interest denoted by the positive.
ÔÇó True-positive rate (TPR) gives the proportion of data of class 
i that are also labelled as i by the classifier.
ÔÇó ùëáùëÉùëÖ=
ùëáùëÉ
ùëáùëÉ+ùêπùëÅ
ÔÇó False-positive rate (FPR) ‚Äì proportion of data that do not 
belong to class i but are labelled with that class.
ÔÇó ùêπùëÉùëÖ=
ùêπùëÉ
ùêπùëÉ+ùëáùëÅ
Performance measures for a single 
class of interest 2/2
ÔÇó In the binary case, the two measures can be also defined for the 
opposite, negative class.
ÔÇó True-negative rate ùëáùëÅùëÖ =
ùëáùëÅ
ùëáùëÅ+ùêπùëÉ
ÔÇó False-negative rate ùêπùëÅùëÖ =
ùêπùëÅ
ùêπùëÅ+ùëáùëÉ
ÔÇó TPR is also called sensitivity (or recall).
ÔÇó TNR is also named specificity.
Like

=== 7. Feature selection & parameter tuning.pdf ===
Feature selection & parameter tuning
Ruxandra Stoean
Further bibliography
ÔÇó M. Kuhn, K. Johnson, Feature Engineering and Selection
(Chapman & Hall/CRC Data Science Series), Chapman & 
Hall/CRC Data Science Series, 2021
ÔÇó V . Bol√≥n-Canedo, A. Alonso-Betanzos, Recent Advances in 
Ensembles for Feature Selection, Intelligent Systems Reference
Library, Springer, 2018
ÔÇó E. Bartz, T . Bartz-Beielstein, M. Zaefferer, O. Mersmann, 
Hyperparameter Tuning for Machine and Deep Learning with R: 
A Practical Guide, Springer, 2023
ÔÇó L. Owen, Hyperparameter Tuning with Python: Boost your
machine learning model's performance via hyperparameter 
tuning, Packt Publishing, 2022
Feature selection
Feature selection
ÔÇó Data could have 10 000 descriptive features
ÔÇó Their number should be reduced to 1000 (or 100) to apply a
model
ÔÇó Which 1000 attributes should be kept?
ÔÇó This is feature selection
Features (independent variables) Dependent variable
Why needed?
doi:10.3966/199115592017062803014
ÔÇó Sometimes performance decreases when the number of features 
is high
ÔÇó The curse of dimensionality 
ÔÇó The space volume increases rapidly, data become sparse
ÔÇó More data samples will be needed
Increase in noise
ÔÇó Some additional features can only increase noise
ÔÇó E.g. other features in Pima diabetes: favorite music, hair color
ÔÇó Models could guide learning towards the new attributes
ÔÇó They could find correlations only true for the training set, and 
not generalizable to new data
ÔÇó More features imply more complex models
ÔÇó More weights for a neural network
ÔÇó More nodes in decision trees
ÔÇó More trees in random forests etc.
ÔÇó Increased search space => harder search
Reasons for feature selection
ÔÇó T o improve performance (in terms of accuracy, speed)
ÔÇó For data vizualization, if possible
ÔÇó T o reduce dimensionality and eliminate noise
ÔÇó Simpler models are easier to interpret
ÔÇó Better generalization by reducing overfitting
ÔÇó Variable redundancy
ÔÇó Feature selection is the process of choosing an (almost) opt

=== Deep learning. Convolutional neural networks, transfer, semantic segmentation, generative.pdf ===
Ruxandra Stoean
Deep learning for image processing

Further bibliography
ÔÇó Ian Goodfellow et al, Deep Learning (Adaptive Computation and Machine 
Learning series), MIT Press, 2016 http://www .deeplearningbook.org/
ÔÇó John Kelleher, Deep Learning (MIT Press Essential Knowledge series), 2019
ÔÇó Charu Aggarwal, Neural Networks and Deep Learning: A T extbook 2nd ed, 
Springer, 2023
ÔÇó Aston Zhang et al, Dive into Deep Learning, Cambridge University Press, 
2023
ÔÇó Simon Prince, Understanding Deep Learning, MIT Press, 2023
ÔÇó Jonah Gamba, Deep Learning Models, A Practical Approach for Hands-On 
Professionals, 2024

https://www .kdnuggets.com/2017/08/first-steps-learning-deep-learning-image-classification-keras.html
Definitions
ÔÇó ‚ÄúFor most flavors of the old generations of learning algorithms ‚Ä¶ performance will plateau. ‚Ä¶ deep 
learning ‚Ä¶ is the first class of algorithms ‚Ä¶ that is scalable. ‚Ä¶ performance just keeps getting better as 
you feed them more data.‚Äù - Andrew Ng
ÔÇó ‚ÄúThe hierarchy of concepts allows the computer to learn complicated concepts by building them out of 
simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, 
with many layers. For this reason, we call this approach to AI deep learning.‚Äù ‚Äì Ian Goodfellow
ÔÇó ‚ÄúDeep learning [is] ‚Ä¶ a pipeline of modules all of which are trainable. ‚Ä¶ deep because [has] multiple 
stages in the process of recognizing an object and all of those stages are part of the training‚Äù -Yann LeCun
ÔÇó ‚ÄúAt which problem depth does Shallow Learning end, and Deep Learning begin? Discussions with DL experts 
have not yet yielded a conclusive response to this question. [‚Ä¶], let me just define for the purposes of this 
overview: problems of depth > 10 require Very Deep Learning. ‚Äù - Jurgen Schmidhuber
https://machinelearningmastery.com/what-is-deep-learning/

‚ÄúThe more I read, the more I acquire, 
the more certain I am that I know nothing.‚Äù
‚Äï V oltaire

Frameworks for implementing deep learning


=== Deep learning. Recurrent networks & transformers.pdf ===
Ruxandra Stoean
Deep learning for time series and text processing. 
Recurrent networks and transformers
Further bibliography
ÔÇó A. K.Tyagi, A. Abraham, Recurrent Neural Networks: Concepts and Applications, CRC 
Press, 2022
ÔÇó F. M. Salem, Recurrent Neural Networks: From Simple to Gated Architectures, Springer, 
2022
ÔÇó D. Rothman, Transformers for Natural Language Processing: Build, train, and fine-tune 
deep neural network architectures for NLP with Python, Hugging Face, and OpenAI's 
GPT-3, ChatGPT , and GPT-4, Packt, 2022
ÔÇó S. Ozdemir, Quick Start Guide to Large Language Models: Strategies and Best Practices 
for Using ChatGPT and Other LLMs, Addison-W esley, 2023
ÔÇó S. Raschka, Build a Large Language Model (From Scratch), Manning, 2024
Recurrent networks
The persistence of memory
ÔÇó Traditional neural networks (NN) unable to remember 
information collected over time
ÔÇó Consequent appearance of recurrent neural networks (RNN)
ÔÇó Short-term memory
ÔÇó Long short-term memory networks (LSTM)
ÔÇó Gated recurrent units (GRU)
ÔÇó W eights shared across time
ÔÇó Loss of all time steps is the summed loss at each time step
ÔÇó Backpropagation at each time step, considering the previous 
time steps

LSTM
ÔÇó S. Hochreiter& J. Schmidhuber, Neural Computation, 1997
ÔÇó Applications to speech and text prediction, video action recognition, time series 
prediction
ÔÇó Capable of learning long-term dependencies in the data
ÔÇó Chain-like structure with a repeating module of interacting neural layers
ÔÇó xt ‚Äì input, ht ‚Äì hidden state output
ÔÇó Concentrate on the relevant information through its gates that decide which to keep
https://colah.github.io/
1. The cell state
ÔÇó Similar to a ‚Äúconveyer belt‚Äù where the transport ‚Äúproducts‚Äù = information for the 
network
ÔÇó Information can be added to or removed from the cell state through gates.
https://colah.github.io/
2. Gates
ÔÇó A sigmoid NN layer + a multiplication pointwise operation
ÔÇó The sigmoid has an output in the interval [0, 1] giving the amount of informatio

